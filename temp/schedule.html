<section id="schedule" class="section-with-bg">
      <div class="container wow fadeInUp">
        <div class="section-header">
          <h2>Schedule</h2>
        </div>

        <div role="tabpanel" class="col-lg-9 tab-pane fade show active" id="day-1">
          <div class="row schedule-item justify-content-center">
            <div class="col-md-10 text-center">
              <h4>To Be Determined</h4>
            </div>
          </div>
        

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 8:10-8:35 </time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/nips_logo.jpeg" alt="Una-May O'Reilly">
                </div>
                <h4>Keynote 1</h4>
                <h5><b>(TBD)</b></h5>
                <!-- <h6><b>Title: Adversarial Intelligence Supported by Machine Learning</b></h6> -->
                <!-- <h6><a href="speaker-details.html#Una-May O’Reilly">Abstract & Bio</a></h6> -->
                <!-- <div class="keynote">
                  <button class="collapsible">Bio</button>
                  <div class="content">
                      <p>Una-May O'Reilly is the leader of ALFA Group at MIT-CSAIL. An AI and machine learning researcher for 20+ years, she is  broadly interested in artificial adversarial intelligence  -- the notion that competition has complex dynamics due to learning and adaptation signaled by experiential feedback. This interest directs  her to the topic of security where she has develops machine learning algorithms that variously consider the arms races of malware, network and model attacks and the uses of adversarial inputs on deep learning models.  Her passions are evolutionary computation and programming.  This frequently leads her to investigate Genetic Programming. As well, it draws her to investigations of coevolutionary dynamics between populations of cooperative agents or adversaries, in settings as general as cybersecurity and machine learning.</p>
                  </div>
                  <button class="collapsible">Abstract</button>
                  <div class="content">
                      <p>My interest is in computationally replicating the behavior of adversaries who target algorithms/code/scripts at vulnerable targets and the defenders who try to stop the threats. I typically consider networks as targets but let’s consider the most recent ML models - foundation models. How do goals blur in the current context where the community is trying to simultaneously address their safety and security?</p>
                  </div>
                </div> -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 8:35-9:00 </center></time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/nips_logo.jpeg" alt="Lea Schönherr">
                </div>
                <h4>Keynote 2</h4>
                <h5><b>(TBD)</b></h5>
                <!-- <h6>Title: Brave New World: Challenges and Threats in Multimodal AI Agent Integrations</h6> -->
                <!-- <h6><a href="speaker-details.html#Lea Schönherr">Abstract & Bio</a></h6> -->
                
                <!-- <div class="keynote">
                  <button class="collapsible">Bio</button>
                  <div class="content">
                      <p>Lea Schönherr is a tenure track faculty at CISPA Helmholtz Center for Information Security since 2022. She obtained her PhD from Ruhr-Universität Bochum, Germany, in 2021 and is a recipient of two fellowships from UbiCrypt (DFG Graduate School) and Casa (DFG Cluster of Excellence). Her research interests are in the area of information security with a focus on adversarial machine learning and generative models to defend against real-world threats. She is particularly interested in language as an interface to machine learning models and in combining different domains such as audio, text, and images. She has published several papers on threat detection and defense of speech recognition systems and generative models.</p>
                  </div>
                  <button class="collapsible">Abstract</button>
                  <div class="content">
                      <p>Being on the rise, AI agents become more integrated into our daily lives and will soon be indispensable for countless downstream tasks, be it translation, text enhancing, summarisation or other assisting applications like code generation. As of today, the human-agent interface is no longer limited to plain text and large language models (LLMs) can handle documents, videos, images, audio and more. In addition, the generation of various multimodal outputs is becoming more advanced and realistic in appearance, allowing for more sophisticated communication with AI agents. Particularly in the future, agents will rely on a more natural-feeling voice interface for interactions with AI agents. In this presentation, we will take a closer look at the resulting challenges and security threats associated with integrated multimodal AI agents, which relate to two possible categories: Malicious inputs used to jailbreak LLMs, as well as computer-generated output that is indistinguishable from human-generated content. In the first case, specially designed inputs are used to exploit an LLM or its embedding system, also referred to as prompt hacking. Existing attacks show that content filters of LLMs can be easily bypassed with specific inputs and that private information can be leaked. The use of additional input modalities, such as speech, allows for a much broader potential attack surface that needs to be investigated and protected. In the second case, generative models are utilized to produce fake content that is nearly impossible to distinguish from human-generated content. This fake content is often used for fraudulent and manipulative purposes and impersonation and realistic fake news is already possible using a variety of techniques. As these models continue to evolve, detecting these fraudulent activities will become increasingly difficult, while the attacks themselves will become easier to automate and require less expertise. This creates significant challenges for preventing fraud and the uncontrolled spread of fake news.</p>
                  </div>
                </div> -->

              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 9:00-10:00 </center></time></div>
              <div class="col-md-10">
                <h4>Oral, Spotlight Paper & Demo Presentation</h4>
                <!-- <b>Adversarial Training Should Be Cast as a Non-Zero-Sum Game</b>
                <p>Alexander Robey, Fabian Latorre, George J. Pappas, Hamed Hassani, Volkan Cevher</p> -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 10:00-10:30 </center></time></div>
              <div class="col-md-10">
                <h4>Poster Setup + Coffee Break</h4>
                <!-- <b>Evading Black-box Classifiers Without Breaking Eggs</b>
                <p>Edoardo Debenedetti, Nicholas Carlini, Florian Tramèr</p> --> 
              </div>
            </div>

            <!-- <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 10:20-10:30 </center></time></div>
              <div class="col-md-10">
                <h4>Oral Paper Presentation 3</h4>
                <b>Tunable Dual-Objective GANs for Stable Training</b>
                <p>Monica Welfert, Kyle Otstot, Gowtham Raghunath Kurri, Lalitha Sankar</p>
              </div>
            </div> --> 

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 10:30-11:00 </center></time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/nips_logo.jpeg" alt="Jihun Hamm">
                </div>
                <h4>Keynote 3</h4>
                <h5><b>(TBD)</b></h5>
                <!-- <h6><b>Title: Analyzing Transfer Learning Bounds through Distributional Robustness</b></h6> -->
                <!-- <h6><a href="speaker-details.html#Jihun Hamm">Abstract & Bio</a></h6> -->
                <!--
                <div class="keynote">
                  <button class="collapsible">Bio</button>
                  <div class="content">
                      <p>Dr. Jihun Hamm has been an Associate Professor of Computer Science at Tulane University since 2019. He received his PhD degree from the University of Pennsylvania in 2008 supervised by Dr. Daniel Lee. Dr. Hamm's research interest is in machine learning, from theory and to applications. He has worked on the theory and practice of robust learning, adversarial learning, privacy and security, optimization, and deep learning. Dr. Hamm also has a background in biomedical engineering and has worked on machine learning applications in medical data analysis. His work in machine learning has been published in top venues such as ICML, NeurIPS, CVPR, JMLR, and IEEE-TPAMI. His work has also been published in medical research venues such as MICCAI, MedIA, and IEEE-TMI. Among other awards, he has earned the Best Paper Award from MedIA, Finalist for MICCAI Young Scientist Publication Impact Award, and Google Faculty Research Award.</p>
                  </div>
                  <button class="collapsible">Abstract</button>
                  <div class="content">
                      <p>The success of transfer learning at improving performance, especially with the use of large pre-trained models has made transfer learning an essential tool in the machine learning toolbox. However, the conditions under which performance transferability to downstream tasks is possible are not very well understood. In this talk, I will present several approaches to bounding the target-domain classification loss through distribution shift between the source and the target domains. For domain adaptation/generalization problems where the source and the target task are the same, distribution shift as measured by Wasserstein distance is sufficient to predict the loss bound. Furthermore, distributional robustness improves predictability (i.e., low bound) which may come at the price of performance decrease. For transfer learning where the source and the target task are difference, distributions cannot be compared directly. We therefore propose an simple approach that transforms the source distribution (and classifier) by changing the class prior, label, and feature spaces. This allows us to relate the loss of the downstream task (i.e., transferability) to that of the source task. Wasserstein distance again plays an important role in the bound. I will show empirical results using state-of-the-art pre-trained models, and demonstrate how factors such as task relatedness, pretraining method, and model architecture affect transferability.</p>
                  </div>
                </div> --> 
              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 11:00-12:00 </center></time></div>
              <div class="col-md-10">
                <h4>Poster + Demo Session</h4>
                <p>(for all accepted papers)</p>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 12:00-13:00 </center></time></div>
              <div class="col-md-10">
                <h4>Lunch Break</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 13:00-13:30 </center></time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/nips_logo.jpeg" alt="Atlas Wang">
                </div>
                <h4>Keynote 4</h4>
                <h5><b>(TBD)</b></h5>
                <!-- <h6><b>Title: On the Complicate Romance between Sparsity and Robustness</b></h6> -->
                <!-- <h6><a href="speaker-details.html#Atlas Wang">Abstract & Bio</a></h6> -->
                <!--
                <div class="keynote">
                  <button class="collapsible">Bio</button>
                  <div class="content">
                      <p>Atlas Wang teaches and researches at UT Austin ECE (primary), CS, and Oden CSEM. He usually declares his research interest as machine learning, but is never too sure what that means concretely. He has won some awards, but is mainly proud of just three things: (1) he has done some (hopefully) thought-invoking and practically meaningful work on sparsity, from inverse problems to deep learning; his recent favorites include “essential sparsity”, “junk DNA hypothesis”, and “heavy-hitter oracle”; (2) he co-founded the Conference on Parsimony and Learning (CPAL), known as the new " conference for sparsity" to its community, and serves as its inaugural program chair; (3) he is fortunate enough to work with a sizable group of world-class students, who are all smarter than himself. He has graduated 10 Ph.D. students that are well placed, including two new assistant professors; and his students have altogether won seven PhD fellowships besides many other honors.</p>
                  </div>
                  <button class="collapsible">Abstract</button>
                  <div class="content">
                      <p>Prior arts have observed that appropriate sparsity (or pruning) can improve the empirical robustness of deep neural networks (NNs). In this talk, I will introduce our recent findings extending this line of research. We have firstly demonstrated that sparsity can be injected into adversarial training, either statically or dynamically, to reduce the robust generalization gap besides significantly saving training and inference FLOPs. We then show that pruning can also improve certified robustness for ReLU-based NNs at scale, under the complete verification setting. Lastly, we theoretically characterize the complicated relationship between neural network sparsity and generalization. It is revealed that, as long as the pruning fraction is below a certain threshold, gradient descent can drive the training loss toward zero and the network exhibits good generalization. Meanwhile, there also exists a large pruning fraction such that while gradient descent is still able to drive the training loss toward zero (by memorizing noise), the generalization performance is no better than random guessing.</p>
                  </div>
                </div>
                -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 13:30-14:00 </center></time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/nips_logo.jpeg" alt="Stacy Hobson">
                </div>
                <h4>Keynote 5</h4>
                <h5><b>(TBD)</b></h5>
                <!-- <h6><b>Title: Addressing technology-mediated social harms</b></h6> -->
                <!--
                <div class="keynote">
                  <button class="collapsible">Bio</button>
                  <div class="content">
                      <p>Dr. Stacy Hobson is a Research Scientist at IBM Research and is the Director of the Responsible and Inclusive Technologies research group. Her group’s research focuses on anticipating and understanding the impacts of technology on society and promoting tech practices that minimize harms, biases and other negative outcomes.  Stacy’s research has spanned multiple areas including topics such as addressing social inequities through technology, AI transparency, and data sharing platforms for governmental crisis management. Stacy has authored more than 20 peer-reviewed publications and holds 15 US patents. Stacy earned a Bachelor of Science degree in Computer Science from South Carolina State University, a Master of Science degree in Computer Science from Duke University and a PhD in Neuroscience and Cognitive Science from the University of Maryland at College Park.</p>
                  </div>
                  <button class="collapsible">Abstract</button>
                  <div class="content">
                      <p>Many technology efforts focus almost exclusively on the expected benefits that the resulting innovations may provide.  Although there has been increased attention in past years on topics such as ethics, privacy, fairness and trust in AI, there still exists a wide gap between the aims of responsible innovation and what is occurring most often in practice.  In this talk, I highlight the critical importance of proactively considering technology use in society, with focused attention on societal stakeholders, social impacts and socio-historical context, as the necessary foundation to anticipate and mitigate tech harms.</p>
                  </div>
                </div>
              -->
                <!-- <h6><a href="speaker-details.html#Stacy Hobson">Abstract & Bio</a></h6> -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 14:00-14:45 </center></time></div>
              <div class="col-md-10">
                <h4>Oral, Spotlight Paper & Demo Presentation</h4>
                <!-- <b>Oral, Spotlight Paper & Demo Presentation</b> -->
                <!-- <p>Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, Prateek Mittal</p> -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 14:45-15:00 </center></time></div>
              <div class="col-md-10">
                <h4>Poster Discussion + Coffee Break</h4>
                <!-- <b>Poster Discussion + Coffee Break</b> --> 
                <!-- <p>Konwoo Kim, Gokul Swamy, Zuxin Liu, Ding Zhao, Sanjiban Choudhury, Steven Wu</p> -->
              </div>
            </div>

            <!-- <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 14:20-14:25 </center></time></div>
              <div class="col-md-10">
                <h4>Blue Sky Idea Oral Presentation 1</h4>
                <b>MLSMM: Machine Learning Security Maturity Model</b>
                <p>Felix Viktor Jedrzejewski, Davide Fucci, Oleksandr Adamov</p>
              </div>
            </div> -->

            <!-- <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 14:25-14:30 </center></time></div>
              <div class="col-md-10">
                <h4>Blue Sky Idea Oral Presentation 2</h4>
                <b>Deceptive Alignment Monitoring</b>
                <p>Dhruv Bhandarkar Pai, Andres Carranza, Arnuv Tandon, Rylan Schaeffer, Sanmi Koyejo</p>
              </div>
            </div> -->

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 15:00-15:30 </center></time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/nips_logo.jpeg" alt="Aditi Raghunathan">
                </div>
                <h4>Keynote 6</h4>
                <h5><b>(TBD)</b></h5>
                <!-- <h6><b>Title: Beyond Adversaries: Robustness to Distribution Shifts in the Wild</b></h6>
                <div class="keynote">
                  <button class="collapsible">Bio</button>
                  <div class="content">
                      <p>Aditi Raghunathan is an Assistant Professor at Carnegie Mellon University. She is interested in building robust ML systems with guarantees for trustworthy real-world deployment. Previously, she was a postdoctoral researcher at Berkeley AI Research, and received her PhD from Stanford University in 2021. Her research has been recognized by the Schmidt AI2050 Early Career Fellowship, the Arthur Samuel Best Thesis Award at Stanford, a Google PhD fellowship in machine learning, and an Open Philanthropy AI fellowship.</p>
                  </div>
                  <button class="collapsible">Abstract</button>
                  <div class="content">
                      <p>Machine learning systems often fail catastrophically under the presence of distribution shift—when the test distribution differs in some systematic way from the training distribution. Such shifts can sometimes be captured via an adversarial threat model, but in many cases, there is no convenient threat model that appropriately captures the “real-world” distribution shift. In this talk, we will first discuss how to measure the robustness to such distribution shifts despite the apparent lack of structure. Next, we discuss how to improve robustness to such shifts. The past few years have seen the rise of large models trained on broad data at scale that can be adapted to several downstream tasks (e.g. BERT, GPT, DALL-E). Via theory and experiments, we will see how such models open up new avenues but also require new techniques for improving robustness.</p>
                  </div>
                </div> -->
                <!-- <h6><a href="speaker-details.html#Aditi Raghunathan">Abstract & Bio</a></h6> -->
              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 15:30-16:00 </center></time></div>
              <div class="col-md-10">
                <h4>Panel discussion</h4>
                <!-- <b>How Can Neuroscience Help Us Build More Robust Deep Neural Networks?</b>
                <p>Michael Teti, Garrett T. Kenyon, Juston Moore</p> -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 16:00-17:00 </center></time></div>
              <div class="col-md-10">
                <h4>Award and Poster</h4>
                <p>(for all accepted papers)</p>
              </div>
            </div>

            <!-- <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 15:40-15:45 </center></time></div>
              <div class="col-md-10">
                <h4>Announcement of AdvML Rising Star Award</h4>
              </div>
            </div> -->

            <!-- <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 15:45-16:00 </center></time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/awardees/tianlong.jpg" alt="Tianlong Chen">
                </div>
                <h4>Award Presentation 1
                  
                </h4>
                <p>Tianlong Chen</p>
              </div>
            </div> -->

            <!-- <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 16:00-16:15 </center></time></div>
              <div class="col-md-10">
                <div class="speaker">
                  <img src="img/awardees/vikash.jpg" alt="Vikash Sehwag">
                </div>
                <h4>Award Presentation 2
                  
                </h4>
                <p>Vikash Sehwag</p>
              </div>
            </div> -->

            <!-- <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 16:15-17:00 </center></time></div>
              <div class="col-md-10">
                <h4>Poster Session & Closing Remarks</h4>
                <p>(for all accepted papers)</p>
              </div>
            </div> -->

            <div class="row schedule-item">
              <div class="col-md-2"><time><center>Time: <br> 17:00-17:10</time></div>
              <div class="col-md-10">
                <h4>Closing Remarks</h4>
              </div>
            </div>

        </div>
      
      
      </div>

    </section>
    