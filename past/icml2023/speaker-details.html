<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>AdvML 2023 - New Frontiers in Adversarial Machine Learning</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/ICML.jpeg" rel="icon">
  <link href="img/ICML.jpeg" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">
</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header" class="header-fixed">
    <div class="container">

      <div id="logo" class="pull-left">
        <!-- Uncomment below if you prefer to use a text logo -->
        <!-- <h1><a href="#main">C<span>o</span>nf</a></h1>-->
        <!-- <a href="index.html#intro" class="scrollto"><img src="img/logo.png" alt="" title=""></a> -->
        <a href="index.html#intro" class="scrollto"><p style="font-size:30px;color:white">AdvML-Frontiers 2023</p></a>
      </div>

      <nav id="nav-menu-container" class="navbar navbar-default navbar-fixed-top">
        <ul class="nav-menu">
          <li class="menu-active"><a class="nav-link scrollto active" href="#intro">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#speakers">Speakers</a></li>
          <li><a class="nav-link scrollto" href="#schedule">Schedule</a></li>
          <li><a class="nav-link scrollto" href="#organizers">Contacts</a></li>
          <li><a class="nav-link scrollto" href="./past/icml2022">AdvML-Frontiers'22</a></li>
          <li><a class="btn navbar-btn mx-2 text-white btn-outline-light" href="#" target="_blank" rel="noopener noreferrer">SUBMIT</a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->

  <main id="main" class="main-page">

    <!--==========================
      Speaker Details Section
    ============================-->
    <section id="speakers-details" class="wow fadeIn">
      <div class="container">
        <div class="section-header">
          <h2>Speaker Details</h2>
        </div>

        
        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Reilly.jpg" alt="Una-May O’Reilly" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Una-May O’Reilly"><a href="http://people.csail.mit.edu/unamay/" target="_blank" rel="noopener noreferrer">Una-May O’Reilly</a></h2>
              <p>Massachusetts Institute of Technology, USA</p>
              <p><b>Bio:</b><br>Una-May O'Reilly is the leader of ALFA Group at MIT-CSAIL. An AI and machine learning researcher for 20+ years, she is  broadly interested in artificial adversarial intelligence  -- the notion that competition has complex dynamics due to learning and adaptation signaled by experiential feedback. This interest directs  her to the topic of security where she has develops machine learning algorithms that variously consider the arms races of malware, network and model attacks and the uses of adversarial inputs on deep learning models.  Her passions are evolutionary computation and programming.  This frequently leads her to investigate Genetic Programming. As well, it draws her to investigations of coevolutionary dynamics between populations of cooperative agents or adversaries, in settings as general as cybersecurity and machine learning.</p>
              <p><b>Keynote Title:</b><br>Adversarial Intelligence Supported by Machine Learning</p>
			        <p><b>Keynote Abstract:</b><br>My interest is in computationally replicating the behavior of adversaries who target algorithms/code/scripts at vulnerable targets and the defenders who try to stop the threats. I typically consider networks as targets but let’s consider the most recent ML models - foundation models. How do goals blur in the current context where the community is trying to simultaneously address their safety and security?</p> 
			  <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>


        <p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Zico.jpg" alt="Zico Kolter" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Zico Kolter"><a href="https://zicokolter.com/" target="_blank" rel="noopener noreferrer">Zico Kolter</a></h2>
              <p>CMU, USA</p>
              <p><b>Bio:</b><br>Zico Kolter is an Associate Professor in the Computer Science Department at Carnegie Mellon University, and also serves as chief scientist of AI research for the Bosch Center for Artificial Intelligence. His work spans the intersection of machine learning and optimization, with a large focus on developing more robust and rigorous methods in deep learning. In addition, he has worked in a number of application areas, highlighted by work on sustainability and smart energy systems. He is a recipient of the DARPA Young Faculty Award, a Sloan Fellowship, and best paper awards at NeurIPS, ICML (honorable mention), AISTATS (test of time), IJCAI, KDD, and PESGM.</p>
			  <p><b>Keynote Title:</b><br>TBD</p>
        <p><b>Keynote Abstract:</b><br>TBD</p> 
			  <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
		
		
		<p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Kamalika.jpg" alt="Kamalika Chaudhuri" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Kamalika Chaudhuri"><a href="img/speakers/Kamalika.jpg" target="_blank" rel="noopener noreferrer">Kamalika Chaudhuri</a></h2>
              <p>UCSD, USA</p>
            <p><b>Bio:</b><br>Kamalika Chaudhuri is a Professor in the department of Computer Science and Engineering at University of California San Diego, and a Research Scientist in the FAIR team at Meta AI. Her research interests are in the foundations of trustworthy machine learning, which includes problems such as learning from sensitive data while preserving privacy, learning under sampling bias, and in the presence of an adversary. She is particularly interested in privacy-preserving machine learning, which addresses how to learn good models and predictors from sensitive data, while preserving the privacy of individuals. </p>
			      <p><b>Keynote Title:</b><br>Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning</p>
	          <p><b>Keynote Abstract:</b><br>Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize specific parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models -- which we refer to as déjà vu memorization. Concretely, we show that given the trained model and a crop of a training image containing only the background (e.g., water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that déjà vu memorization is common to different SSL algorithms, is exacerbated by certain design choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of déjà vu memorization reveals previously unknown privacy risks in SSL models, as well as suggests potential practical mitigation strategies.</p>
            <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>

      <p></p>

      <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Lea.jpg" alt="Lea Schönherr" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Lea Schönherr"><a href="https://leaschoenherr.me/" target="_blank" rel="noopener noreferrer">Lea Schönherr</a></h2>
              <p>Helmholtz Center, Germany</p>
            <p><b>Bio:</b><br>Lea Schönherr is a tenure track faculty at CISPA Helmholtz Center for Information Security since 2022. She obtained her PhD from Ruhr-Universität Bochum, Germany, in 2021 and is a recipient of two fellowships from UbiCrypt (DFG Graduate School) and Casa (DFG Cluster of Excellence). Her research interests are in the area of information security with a focus on adversarial machine learning and generative models to defend against real-world threats. She is particularly interested in language as an interface to machine learning models and in combining different domains such as audio, text, and images. She has published several papers on threat detection and defense of speech recognition systems and generative models.</p>
            <p><b>Keynote Title:</b><br>Brave New World: Challenges and Threats in Multimodal AI Agent Integrations</p>
            <p><b>Keynote Abstract:</b><br>Being on the rise, AI agents become more integrated into our daily lives and will soon be indispensable for countless downstream tasks, be it translation, text enhancing, summarisation or other assisting applications like code generation. As of today, the human-agent interface is no longer limited to plain text and large language models (LLMs) can handle documents, videos, images, audio and more. In addition, the generation of various multimodal outputs is becoming more advanced and realistic in appearance, allowing for more sophisticated communication with AI agents. Particularly in the future, agents will rely on a more natural-feeling voice interface for interactions with AI agents. In this presentation, we will take a closer look at the resulting challenges and security threats associated with integrated multimodal AI agents, which relate to two possible categories: Malicious inputs used to jailbreak LLMs, as well as computer-generated output that is indistinguishable from human-generated content. In the first case, specially designed inputs are used to exploit an LLM or its embedding system, also referred to as prompt hacking. Existing attacks show that content filters of LLMs can be easily bypassed with specific inputs and that private information can be leaked. The use of additional input modalities, such as speech, allows for a much broader potential attack surface that needs to be investigated and protected. In the second case, generative models are utilized to produce fake content that is nearly impossible to distinguish from human-generated content. This fake content is often used for fraudulent and manipulative purposes and impersonation and realistic fake news is already possible using a variety of techniques. As these models continue to evolve, detecting these fraudulent activities will become increasingly difficult, while the attacks themselves will become easier to automate and require less expertise. This creates significant challenges for preventing fraud and the uncontrolled spread of fake news.</p> 
            <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
    
    
      <p></p>

      <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Stacy.jpg" alt="Stacy Hobson" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Stacy Hobson"><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-stacypre" target="_blank" rel="noopener noreferrer">Stacy Hobson</a></h2>
              <p>The University of Western Australia, Australia</p>
              <p><b>Bio:</b><br>Dr. Stacy Hobson is a Research Scientist at IBM Research and is the Director of the Responsible and Inclusive Technologies research group. Her group’s research focuses on anticipating and understanding the impacts of technology on society and promoting tech practices that minimize harms, biases and other negative outcomes.  Stacy’s research has spanned multiple areas including topics such as addressing social inequities through technology, AI transparency, and data sharing platforms for governmental crisis management. Stacy has authored more than 20 peer-reviewed publications and holds 15 US patents. Stacy earned a Bachelor of Science degree in Computer Science from South Carolina State University, a Master of Science degree in Computer Science from Duke University and a PhD in Neuroscience and Cognitive Science from the University of Maryland at College Park.</p>
              <p><b>Keynote Title:</b><br>TBD</p>
              <p><b>Keynote Abstract:</b><br>TBD</p> 
        <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
    
      <p></p>

      <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Juhun.jpg" alt="Jihun Hamm" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Jihun Hamm"><a href="https://sse.tulane.edu/cs/faculty/hamm" target="_blank" rel="noopener noreferrer">Jihun Hamm</a></h2>
              <p>Tulane University, USA</p>
              <p><b>Bio:</b><br>Dr. Jihun Hamm has been an Associate Professor of Computer Science at Tulane University since 2019. He received his PhD degree from the University of Pennsylvania in 2008 supervised by Dr. Daniel Lee. Dr. Hamm's research interest is in machine learning, from theory and to applications. He has worked on the theory and practice of robust learning, adversarial learning, privacy and security, optimization, and deep learning. Dr. Hamm also has a background in biomedical engineering and has worked on machine learning applications in medical data analysis. His work in machine learning has been published in top venues such as ICML, NeurIPS, CVPR, JMLR, and IEEE-TPAMI. His work has also been published in medical research venues such as MICCAI, MedIA, and IEEE-TMI. Among other awards, he has earned the Best Paper Award from MedIA,  Finalist for MICCAI Young Scientist Publication Impact Award, and Google Faculty Research Award.</p>
              <p><b>Keynote Title:</b><br>Analyzing Transfer Learning Bounds through Distributional Robustness</p>
              <p><b>Keynote Abstract:</b><br>The success of transfer learning at improving performance, especially with the use of large pre-trained models has made transfer learning an essential tool in the machine learning toolbox. However, the conditions under which performance transferability to downstream tasks is possible are not very well understood. In this talk, I will present several approaches to bounding the target-domain classification loss through distribution shift between the source and the target domains. For domain adaptation/generalization problems where the source and the target task are the same, distribution shift as measured by Wasserstein distance is sufficient to predict the loss bound. Furthermore, distributional robustness improves predictability (i.e., low bound) which may come at the price of performance decrease. For transfer learning where the source and the target task are difference, distributions cannot be compared directly. We therefore propose an simple approach that transforms the source distribution (and classifier) by changing the class prior, label, and feature spaces. This allows us to relate the loss of the downstream task (i.e., transferability) to that of the source task. Wasserstein distance again plays an important role in the bound. I will show empirical results using state-of-the-art pre-trained models, and demonstrate how factors such as task relatedness, pretraining method, and model architecture affect transferability.</p> 
            <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
    
      <p></p>

      <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Aditi.jpg" alt="Aditi Raghunathan" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Aditi Raghunathan"><a href="https://www.cs.cmu.edu/~aditirag/" target="_blank" rel="noopener noreferrer">Aditi Raghunathan</a></h2>
              <p>CMU, USA</p>
              <p><b>Bio:</b><br>Aditi Raghunathan is an Assistant Professor at Carnegie Mellon University. She is interested in building robust ML systems with guarantees for trustworthy real-world deployment. Previously, she was a postdoctoral researcher at Berkeley AI Research, and received her PhD from Stanford University in 2021. Her research has been recognized by the Schmidt AI2050 Early Career Fellowship, the Arthur Samuel Best Thesis Award at Stanford, a Google PhD fellowship in machine learning, and an Open Philanthropy AI fellowship.</p>
              <p><b>Keynote Title:</b><br>TBD</p>
              <p><b>Keynote Abstract:</b><br>TBD</p> 
              <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
    
      <p></p>

      <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Atlas.jpg" alt="Atlas Wang" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Atlas Wang"><a href="https://vita-group.github.io/index.html" target="_blank" rel="noopener noreferrer">Atlas Wang</a></h2>
              <p>UT Austin, USA</p>
              <p><b>Bio:</b><br>Atlas Wang (https://vita-group.github.io/) teaches and researches at UT Austin ECE (primary), CS, and Oden CSEM. He usually declares his research interest as machine learning, but is never too sure what that means concretely. He has won some awards, but is mainly proud of just three things: (1) he has done some (hopefully) thought-invoking and practically meaningful work on sparsity, from inverse problems to deep learning; his recent favorites include “essential sparsity”, “junk DNA hypothesis”, and “heavy-hitter oracle”; (2) he co-founded the Conference on Parsimony and Learning (CPAL), known as the new " conference for sparsity" to its community, and serves as its inaugural program chair; (3) he is fortunate enough to work with a sizable group of world-class students, who are all smarter than himself. He has graduated 10 Ph.D. students that are well placed, including two new assistant professors; and his students have altogether won seven PhD fellowships besides many other honors.</p>
              <p><b>Keynote Title:</b><br>On the Complicate Romance between Sparsity and Robustness</p>
              <p><b>Keynote Abstract:</b><br>Prior arts have observed that appropriate sparsity (or pruning) can improve the empirical robustness of deep neural networks (NNs). In this talk, I will introduce our recent findings extending this line of research. We have firstly demonstrated that sparsity can be injected into adversarial training, either statically or dynamically, to reduce the robust generalization gap besides significantly saving training and inference FLOPs. We then show that pruning can also improve certified robustness for ReLU-based NNs at scale, under the complete verification setting. Lastly, we theoretically characterize the complicated relationship between neural network sparsity and generalization. It is revealed that, as long as the pruning fraction is below a certain threshold, gradient descent can drive the training loss toward zero and the network exhibits good generalization. Meanwhile, there also exists a large pruning fraction such that while gradient descent is still able to drive the training loss toward zero (by memorizing noise), the generalization performance is no better than random guessing.</p> 
            <!-- <a href="index.html">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
    
      <p></p>

      </div>

    </section>

  </main>


  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong>TheEvent & AIBSD 2022</strong>. All Rights Reserved
      </div>
      <div class="credits">
        Designed by <a href="https://bootstrapmade.com/" target="_blank" rel="noopener noreferrer">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>
